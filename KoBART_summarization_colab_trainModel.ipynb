{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19de5d89ec96449ba4f99d382fb45657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_11a5dd52e0994989b5370c880c29da79",
            "style": "IPY_MODEL_8507a59e70624bf89091086c989e5f05",
            "tooltip": ""
          }
        },
        "11a5dd52e0994989b5370c880c29da79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8507a59e70624bf89091086c989e5f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#⭐️ KoBART-summarization\n",
        "- [MY Git](https://github.com/ugiugi0823)\n",
        "\n",
        "\n",
        "<!-- [![GitHub Repo stars](https://github.com/ugiugi0823/KoBART_summarization_colab_debug?style=social)](https://github.com/ugiugi0823/KoBART_summarization_colab_debug) -->\n",
        "\n"
      ],
      "metadata": {
        "id": "uAnxTi7iUgFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐️⭐️ Ready for data, install"
      ],
      "metadata": {
        "id": "bKYvCikv_CXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ugiugi0823/KoBART-summarization.git\n",
        "!pip install pytorch-lightning==1.9.0\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "jhwmpi6a7pI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KoBART-summarization/data\n",
        "!gdown '17rA21yHFJjpC8cBMN_j4XQDA8UhHNYgw&confirm=t'\n",
        "!gdown '1IppX5Syt1JLc_aznw5nChTf7XMxj371Y&confirm=t'\n",
        "!tar -xvzf train.tar.gz\n",
        "!tar -xvzf test.tar.gz"
      ],
      "metadata": {
        "id": "rdF3-L7O--nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐️⭐️ Model train\n",
        "- 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥\n",
        "- 🔥(Caution) It took 17 hours!🔥🔥🔥🔥🔥🔥 \n",
        "- 🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥🔥"
      ],
      "metadata": {
        "id": "E4aHWr0e9qlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yoa0kReK_a6B",
        "outputId": "1aba04c6-d591-4b21-fd91-c0269ca3f9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KoBART-summarization\n",
        "CMD = '''python train.py --gradient_clip_val 1.0 \\\n",
        "                      --max_epochs 10 \\\n",
        "                      --default_root_dir /content/KoBART-summarization/logs \\\n",
        "                      --gpus 1 \\\n",
        "                      --batch_size 8 \\\n",
        "                      --num_workers 4 \\\n",
        "                      --lr 3e-6\n",
        "'''\n",
        "!{CMD}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRF7QIie892D",
        "outputId": "ee03a2aa-b38e-4bbb-deb8-a8c839240928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoBART-summarization\n",
            "2023-03-30 05:35:33.061227: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-30 05:35:33.061319: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-30 05:35:33.061336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "INFO:root:Namespace(checkpoint_path=None, batch_size=8, lr=3e-06, warmup_ratio=0.1, model_path=None, train_file='data/train.tsv', test_file='data/test.tsv', max_len=512, num_workers=4, logger=True, enable_checkpointing=True, default_root_dir='/content/KoBART-summarization/logs', gradient_clip_val=1.0, gradient_clip_algorithm=None, num_nodes=1, num_processes=None, devices=None, gpus=1, auto_select_gpus=None, tpu_cores=None, ipus=None, enable_progress_bar=True, overfit_batches=0.0, track_grad_norm=-1, check_val_every_n_epoch=1, fast_dev_run=False, accumulate_grad_batches=None, max_epochs=10, min_epochs=None, max_steps=-1, min_steps=None, max_time=None, limit_train_batches=None, limit_val_batches=None, limit_test_batches=None, limit_predict_batches=None, val_check_interval=None, log_every_n_steps=50, accelerator=None, strategy=None, sync_batchnorm=False, precision=32, enable_model_summary=True, num_sanity_val_steps=2, resume_from_checkpoint=None, profiler=None, benchmark=None, reload_dataloaders_every_n_epochs=0, auto_lr_find=False, replace_sampler_ddp=True, detect_anomaly=False, auto_scale_batch_size=False, plugins=None, amp_backend=None, amp_level=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', inference_mode=True)\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /content/KoBART-summarization/logs exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "INFO:root:number of workers 4, data length 4281\n",
            "INFO:root:num_train_steps : 1337\n",
            "INFO:root:num_warmup_steps : 133\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "495.440   Total estimated model params size (MB)\n",
            "Epoch 0:  80% 4280/5352 [1:20:02<20:02,  1.12s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  80% 4300/5352 [1:20:10<19:36,  1.12s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  81% 4320/5352 [1:20:18<19:11,  1.12s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  81% 4340/5352 [1:20:26<18:45,  1.11s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  81% 4360/5352 [1:20:34<18:20,  1.11s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  82% 4380/5352 [1:20:42<17:54,  1.11s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  82% 4400/5352 [1:20:51<17:29,  1.10s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  83% 4420/5352 [1:20:59<17:04,  1.10s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  83% 4440/5352 [1:21:07<16:39,  1.10s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  83% 4460/5352 [1:21:15<16:15,  1.09s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  84% 4480/5352 [1:21:23<15:50,  1.09s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  84% 4500/5352 [1:21:31<15:26,  1.09s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  84% 4520/5352 [1:21:39<15:01,  1.08s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  85% 4540/5352 [1:21:47<14:37,  1.08s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  85% 4560/5352 [1:21:55<14:13,  1.08s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  86% 4580/5352 [1:22:03<13:49,  1.08s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  86% 4600/5352 [1:22:11<13:26,  1.07s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  86% 4620/5352 [1:22:19<13:02,  1.07s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  87% 4640/5352 [1:22:27<12:39,  1.07s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  87% 4660/5352 [1:22:36<12:15,  1.06s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  87% 4680/5352 [1:22:44<11:52,  1.06s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  88% 4700/5352 [1:22:52<11:29,  1.06s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  88% 4720/5352 [1:23:00<11:06,  1.06s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  89% 4740/5352 [1:23:08<10:44,  1.05s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  89% 4760/5352 [1:23:16<10:21,  1.05s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  89% 4780/5352 [1:23:24<09:58,  1.05s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  90% 4800/5352 [1:23:32<09:36,  1.04s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  90% 4820/5352 [1:23:40<09:14,  1.04s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  90% 4840/5352 [1:23:48<08:51,  1.04s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  91% 4860/5352 [1:23:56<08:29,  1.04s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  91% 4880/5352 [1:24:04<08:07,  1.03s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  92% 4900/5352 [1:24:12<07:46,  1.03s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  92% 4920/5352 [1:24:20<07:24,  1.03s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  92% 4940/5352 [1:24:28<07:02,  1.03s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  93% 4960/5352 [1:24:36<06:41,  1.02s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  93% 4980/5352 [1:24:45<06:19,  1.02s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  93% 5000/5352 [1:24:53<05:58,  1.02s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  94% 5020/5352 [1:25:01<05:37,  1.02s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  94% 5040/5352 [1:25:09<05:16,  1.01s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  95% 5060/5352 [1:25:17<04:55,  1.01s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  95% 5080/5352 [1:25:25<04:34,  1.01s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  95% 5100/5352 [1:25:33<04:13,  1.01s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  96% 5120/5352 [1:25:41<03:52,  1.00s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  96% 5140/5352 [1:25:49<03:32,  1.00s/it, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  96% 5160/5352 [1:25:57<03:11,  1.00it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  97% 5180/5352 [1:26:05<02:51,  1.00it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  97% 5200/5352 [1:26:13<02:31,  1.01it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  98% 5220/5352 [1:26:21<02:11,  1.01it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  98% 5240/5352 [1:26:29<01:50,  1.01it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  98% 5260/5352 [1:26:37<01:30,  1.01it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  99% 5280/5352 [1:26:45<01:10,  1.01it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  99% 5300/5352 [1:26:53<00:51,  1.02it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0:  99% 5320/5352 [1:27:02<00:31,  1.02it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Epoch 0: 100% 5340/5352 [1:27:10<00:11,  1.02it/s, loss=1.52, v_num=5, train_loss=1.240]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:07<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 0: 100% 5352/5352 [1:27:14<00:00,  1.02it/s, loss=1.54, v_num=5, train_loss=1.720, val_loss=1.470]\n",
            "Epoch 0: 100% 5352/5352 [1:27:14<00:00,  1.02it/s, loss=1.54, v_num=5, train_loss=1.720, val_loss=1.470]Epoch 0, global step 4281: 'val_loss' reached 1.47174 (best 1.47174), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=00-val_loss=1.472.ckpt' as top 3\n",
            "Epoch 1:  80% 4280/5352 [1:20:08<20:04,  1.12s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  80% 4300/5352 [1:20:16<19:38,  1.12s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  81% 4320/5352 [1:20:24<19:12,  1.12s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  81% 4340/5352 [1:20:32<18:46,  1.11s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  81% 4360/5352 [1:20:41<18:21,  1.11s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  82% 4380/5352 [1:20:49<17:56,  1.11s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  82% 4400/5352 [1:20:57<17:30,  1.10s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  83% 4420/5352 [1:21:05<17:05,  1.10s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  83% 4440/5352 [1:21:13<16:41,  1.10s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  83% 4460/5352 [1:21:21<16:16,  1.09s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  84% 4480/5352 [1:21:29<15:51,  1.09s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  84% 4500/5352 [1:21:37<15:27,  1.09s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  84% 4520/5352 [1:21:45<15:02,  1.09s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  85% 4540/5352 [1:21:53<14:38,  1.08s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  85% 4560/5352 [1:22:01<14:14,  1.08s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  86% 4580/5352 [1:22:09<13:50,  1.08s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  86% 4600/5352 [1:22:17<13:27,  1.07s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  86% 4620/5352 [1:22:26<13:03,  1.07s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  87% 4640/5352 [1:22:34<12:40,  1.07s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  87% 4660/5352 [1:22:42<12:16,  1.06s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  87% 4680/5352 [1:22:50<11:53,  1.06s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  88% 4700/5352 [1:22:58<11:30,  1.06s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  88% 4720/5352 [1:23:06<11:07,  1.06s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  89% 4740/5352 [1:23:14<10:44,  1.05s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  89% 4760/5352 [1:23:22<10:22,  1.05s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  89% 4780/5352 [1:23:30<09:59,  1.05s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  90% 4800/5352 [1:23:38<09:37,  1.05s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  90% 4820/5352 [1:23:46<09:14,  1.04s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  90% 4840/5352 [1:23:54<08:52,  1.04s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  91% 4860/5352 [1:24:02<08:30,  1.04s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  91% 4880/5352 [1:24:10<08:08,  1.04s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  92% 4900/5352 [1:24:19<07:46,  1.03s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  92% 4920/5352 [1:24:27<07:24,  1.03s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  92% 4940/5352 [1:24:35<07:03,  1.03s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  93% 4960/5352 [1:24:43<06:41,  1.02s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  93% 4980/5352 [1:24:51<06:20,  1.02s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  93% 5000/5352 [1:24:59<05:58,  1.02s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  94% 5020/5352 [1:25:07<05:37,  1.02s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  94% 5040/5352 [1:25:15<05:16,  1.01s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  95% 5060/5352 [1:25:23<04:55,  1.01s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  95% 5080/5352 [1:25:31<04:34,  1.01s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  95% 5100/5352 [1:25:39<04:13,  1.01s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  96% 5120/5352 [1:25:47<03:53,  1.01s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  96% 5140/5352 [1:25:55<03:32,  1.00s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  96% 5160/5352 [1:26:03<03:12,  1.00s/it, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  97% 5180/5352 [1:26:12<02:51,  1.00it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  97% 5200/5352 [1:26:20<02:31,  1.00it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  98% 5220/5352 [1:26:28<02:11,  1.01it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  98% 5240/5352 [1:26:36<01:51,  1.01it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  98% 5260/5352 [1:26:44<01:31,  1.01it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  99% 5280/5352 [1:26:52<01:11,  1.01it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  99% 5300/5352 [1:27:00<00:51,  1.02it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1:  99% 5320/5352 [1:27:08<00:31,  1.02it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Epoch 1: 100% 5340/5352 [1:27:16<00:11,  1.02it/s, loss=1.51, v_num=5, train_loss=1.550, val_loss=1.470]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:07<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 1: 100% 5352/5352 [1:27:21<00:00,  1.02it/s, loss=1.52, v_num=5, train_loss=1.740, val_loss=1.420]\n",
            "Epoch 1: 100% 5352/5352 [1:27:21<00:00,  1.02it/s, loss=1.52, v_num=5, train_loss=1.740, val_loss=1.420]Epoch 1, global step 8562: 'val_loss' reached 1.41532 (best 1.41532), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=01-val_loss=1.415.ckpt' as top 3\n",
            "Epoch 2:  80% 4280/5352 [1:20:11<20:05,  1.12s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  80% 4300/5352 [1:20:19<19:39,  1.12s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  81% 4320/5352 [1:20:27<19:13,  1.12s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  81% 4340/5352 [1:20:35<18:47,  1.11s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  81% 4360/5352 [1:20:43<18:22,  1.11s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  82% 4380/5352 [1:20:51<17:56,  1.11s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  82% 4400/5352 [1:20:59<17:31,  1.10s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  83% 4420/5352 [1:21:07<17:06,  1.10s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  83% 4440/5352 [1:21:16<16:41,  1.10s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  83% 4460/5352 [1:21:24<16:16,  1.10s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  84% 4480/5352 [1:21:32<15:52,  1.09s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  84% 4500/5352 [1:21:40<15:27,  1.09s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  84% 4520/5352 [1:21:48<15:03,  1.09s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  85% 4540/5352 [1:21:56<14:39,  1.08s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  85% 4560/5352 [1:22:04<14:15,  1.08s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  86% 4580/5352 [1:22:12<13:51,  1.08s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  86% 4600/5352 [1:22:20<13:27,  1.07s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  86% 4620/5352 [1:22:28<13:04,  1.07s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  87% 4640/5352 [1:22:36<12:40,  1.07s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  87% 4660/5352 [1:22:44<12:17,  1.07s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  87% 4680/5352 [1:22:52<11:54,  1.06s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  88% 4700/5352 [1:23:01<11:30,  1.06s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  88% 4720/5352 [1:23:09<11:08,  1.06s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  89% 4740/5352 [1:23:17<10:45,  1.05s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  89% 4760/5352 [1:23:25<10:22,  1.05s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  89% 4780/5352 [1:23:33<09:59,  1.05s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  90% 4800/5352 [1:23:41<09:37,  1.05s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  90% 4820/5352 [1:23:49<09:15,  1.04s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  90% 4840/5352 [1:23:57<08:52,  1.04s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  91% 4860/5352 [1:24:05<08:30,  1.04s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  91% 4880/5352 [1:24:13<08:08,  1.04s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  92% 4900/5352 [1:24:21<07:46,  1.03s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  92% 4920/5352 [1:24:30<07:25,  1.03s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  92% 4940/5352 [1:24:38<07:03,  1.03s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  93% 4960/5352 [1:24:46<06:41,  1.03s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  93% 4980/5352 [1:24:54<06:20,  1.02s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  93% 5000/5352 [1:25:02<05:59,  1.02s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  94% 5020/5352 [1:25:10<05:37,  1.02s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  94% 5040/5352 [1:25:18<05:16,  1.02s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  95% 5060/5352 [1:25:26<04:55,  1.01s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  95% 5080/5352 [1:25:34<04:34,  1.01s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  95% 5100/5352 [1:25:42<04:14,  1.01s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  96% 5120/5352 [1:25:50<03:53,  1.01s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  96% 5140/5352 [1:25:59<03:32,  1.00s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  96% 5160/5352 [1:26:07<03:12,  1.00s/it, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  97% 5180/5352 [1:26:15<02:51,  1.00it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  97% 5200/5352 [1:26:23<02:31,  1.00it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  98% 5220/5352 [1:26:31<02:11,  1.01it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  98% 5240/5352 [1:26:39<01:51,  1.01it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  98% 5260/5352 [1:26:47<01:31,  1.01it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  99% 5280/5352 [1:26:55<01:11,  1.01it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  99% 5300/5352 [1:27:03<00:51,  1.01it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2:  99% 5320/5352 [1:27:11<00:31,  1.02it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Epoch 2: 100% 5340/5352 [1:27:19<00:11,  1.02it/s, loss=1.38, v_num=5, train_loss=1.290, val_loss=1.420]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:08<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 2: 100% 5352/5352 [1:27:24<00:00,  1.02it/s, loss=1.43, v_num=5, train_loss=2.350, val_loss=1.390]\n",
            "Epoch 2: 100% 5352/5352 [1:27:24<00:00,  1.02it/s, loss=1.43, v_num=5, train_loss=2.350, val_loss=1.390]Epoch 2, global step 12843: 'val_loss' reached 1.38727 (best 1.38727), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=02-val_loss=1.387.ckpt' as top 3\n",
            "Epoch 3:  80% 4280/5352 [1:20:10<20:04,  1.12s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  80% 4300/5352 [1:20:18<19:38,  1.12s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  81% 4320/5352 [1:20:26<19:12,  1.12s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  81% 4340/5352 [1:20:34<18:47,  1.11s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  81% 4360/5352 [1:20:42<18:21,  1.11s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  82% 4380/5352 [1:20:50<17:56,  1.11s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  82% 4400/5352 [1:20:58<17:31,  1.10s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  83% 4420/5352 [1:21:06<17:06,  1.10s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  83% 4440/5352 [1:21:14<16:41,  1.10s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  83% 4460/5352 [1:21:22<16:16,  1.09s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  84% 4480/5352 [1:21:30<15:51,  1.09s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  84% 4500/5352 [1:21:38<15:27,  1.09s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  84% 4520/5352 [1:21:46<15:03,  1.09s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  85% 4540/5352 [1:21:54<14:39,  1.08s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  85% 4560/5352 [1:22:03<14:15,  1.08s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  86% 4580/5352 [1:22:11<13:51,  1.08s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  86% 4600/5352 [1:22:19<13:27,  1.07s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  86% 4620/5352 [1:22:27<13:03,  1.07s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  87% 4640/5352 [1:22:35<12:40,  1.07s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  87% 4660/5352 [1:22:43<12:17,  1.07s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  87% 4680/5352 [1:22:51<11:53,  1.06s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  88% 4700/5352 [1:22:59<11:30,  1.06s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  88% 4720/5352 [1:23:07<11:07,  1.06s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  89% 4740/5352 [1:23:15<10:45,  1.05s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  89% 4760/5352 [1:23:23<10:22,  1.05s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  89% 4780/5352 [1:23:31<09:59,  1.05s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  90% 4800/5352 [1:23:40<09:37,  1.05s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  90% 4820/5352 [1:23:48<09:14,  1.04s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  90% 4840/5352 [1:23:56<08:52,  1.04s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  91% 4860/5352 [1:24:04<08:30,  1.04s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  91% 4880/5352 [1:24:12<08:08,  1.04s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  92% 4900/5352 [1:24:20<07:46,  1.03s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  92% 4920/5352 [1:24:28<07:25,  1.03s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  92% 4940/5352 [1:24:36<07:03,  1.03s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  93% 4960/5352 [1:24:44<06:41,  1.03s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  93% 4980/5352 [1:24:52<06:20,  1.02s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  93% 5000/5352 [1:25:00<05:59,  1.02s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  94% 5020/5352 [1:25:08<05:37,  1.02s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  94% 5040/5352 [1:25:17<05:16,  1.02s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  95% 5060/5352 [1:25:25<04:55,  1.01s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  95% 5080/5352 [1:25:33<04:34,  1.01s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  95% 5100/5352 [1:25:41<04:14,  1.01s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  96% 5120/5352 [1:25:49<03:53,  1.01s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  96% 5140/5352 [1:25:57<03:32,  1.00s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  96% 5160/5352 [1:26:05<03:12,  1.00s/it, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  97% 5180/5352 [1:26:13<02:51,  1.00it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  97% 5200/5352 [1:26:21<02:31,  1.00it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  98% 5220/5352 [1:26:29<02:11,  1.01it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  98% 5240/5352 [1:26:37<01:51,  1.01it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  98% 5260/5352 [1:26:45<01:31,  1.01it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  99% 5280/5352 [1:26:53<01:11,  1.01it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  99% 5300/5352 [1:27:02<00:51,  1.01it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3:  99% 5320/5352 [1:27:10<00:31,  1.02it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Epoch 3: 100% 5340/5352 [1:27:18<00:11,  1.02it/s, loss=1.41, v_num=5, train_loss=1.390, val_loss=1.390]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:08<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 3: 100% 5352/5352 [1:27:22<00:00,  1.02it/s, loss=1.42, v_num=5, train_loss=1.340, val_loss=1.370]\n",
            "Epoch 3: 100% 5352/5352 [1:27:22<00:00,  1.02it/s, loss=1.42, v_num=5, train_loss=1.340, val_loss=1.370]Epoch 3, global step 17124: 'val_loss' reached 1.36899 (best 1.36899), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=03-val_loss=1.369.ckpt' as top 3\n",
            "Epoch 4:  80% 4280/5352 [1:20:05<20:03,  1.12s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  80% 4300/5352 [1:20:13<19:37,  1.12s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  81% 4320/5352 [1:20:21<19:11,  1.12s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  81% 4340/5352 [1:20:29<18:46,  1.11s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  81% 4360/5352 [1:20:37<18:20,  1.11s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  82% 4380/5352 [1:20:45<17:55,  1.11s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  82% 4400/5352 [1:20:53<17:30,  1.10s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  83% 4420/5352 [1:21:01<17:05,  1.10s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  83% 4440/5352 [1:21:09<16:40,  1.10s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  83% 4460/5352 [1:21:17<16:15,  1.09s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  84% 4480/5352 [1:21:25<15:51,  1.09s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  84% 4500/5352 [1:21:34<15:26,  1.09s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  84% 4520/5352 [1:21:42<15:02,  1.08s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  85% 4540/5352 [1:21:50<14:38,  1.08s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  85% 4560/5352 [1:21:58<14:14,  1.08s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  86% 4580/5352 [1:22:06<13:50,  1.08s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  86% 4600/5352 [1:22:14<13:26,  1.07s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  86% 4620/5352 [1:22:22<13:03,  1.07s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  87% 4640/5352 [1:22:30<12:39,  1.07s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  87% 4660/5352 [1:22:38<12:16,  1.06s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  87% 4680/5352 [1:22:46<11:53,  1.06s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  88% 4700/5352 [1:22:54<11:30,  1.06s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  88% 4720/5352 [1:23:02<11:07,  1.06s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  89% 4740/5352 [1:23:10<10:44,  1.05s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  89% 4760/5352 [1:23:18<10:21,  1.05s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  89% 4780/5352 [1:23:26<09:59,  1.05s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  90% 4800/5352 [1:23:34<09:36,  1.04s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  90% 4820/5352 [1:23:43<09:14,  1.04s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  90% 4840/5352 [1:23:51<08:52,  1.04s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  91% 4860/5352 [1:23:59<08:30,  1.04s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  91% 4880/5352 [1:24:07<08:08,  1.03s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  92% 4900/5352 [1:24:15<07:46,  1.03s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  92% 4920/5352 [1:24:23<07:24,  1.03s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  92% 4940/5352 [1:24:31<07:02,  1.03s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  93% 4960/5352 [1:24:39<06:41,  1.02s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  93% 4980/5352 [1:24:47<06:20,  1.02s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  93% 5000/5352 [1:24:55<05:58,  1.02s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  94% 5020/5352 [1:25:03<05:37,  1.02s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  94% 5040/5352 [1:25:11<05:16,  1.01s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  95% 5060/5352 [1:25:19<04:55,  1.01s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  95% 5080/5352 [1:25:27<04:34,  1.01s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  95% 5100/5352 [1:25:35<04:13,  1.01s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  96% 5120/5352 [1:25:43<03:53,  1.00s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  96% 5140/5352 [1:25:52<03:32,  1.00s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  96% 5160/5352 [1:26:00<03:12,  1.00s/it, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  97% 5180/5352 [1:26:08<02:51,  1.00it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  97% 5200/5352 [1:26:16<02:31,  1.00it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  98% 5220/5352 [1:26:24<02:11,  1.01it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  98% 5240/5352 [1:26:32<01:50,  1.01it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  98% 5260/5352 [1:26:40<01:30,  1.01it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  99% 5280/5352 [1:26:48<01:11,  1.01it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  99% 5300/5352 [1:26:56<00:51,  1.02it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4:  99% 5320/5352 [1:27:04<00:31,  1.02it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Epoch 4: 100% 5340/5352 [1:27:12<00:11,  1.02it/s, loss=1.38, v_num=5, train_loss=1.470, val_loss=1.370]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:07<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 4: 100% 5352/5352 [1:27:17<00:00,  1.02it/s, loss=1.42, v_num=5, train_loss=2.250, val_loss=1.360]\n",
            "Epoch 4: 100% 5352/5352 [1:27:17<00:00,  1.02it/s, loss=1.42, v_num=5, train_loss=2.250, val_loss=1.360]Epoch 4, global step 21405: 'val_loss' reached 1.35674 (best 1.35674), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=04-val_loss=1.357.ckpt' as top 3\n",
            "Epoch 5:  80% 4280/5352 [1:19:50<19:59,  1.12s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  80% 4300/5352 [1:19:58<19:33,  1.12s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  81% 4320/5352 [1:20:05<19:08,  1.11s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  81% 4340/5352 [1:20:13<18:42,  1.11s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  81% 4360/5352 [1:20:21<18:17,  1.11s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  82% 4380/5352 [1:20:29<17:51,  1.10s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  82% 4400/5352 [1:20:37<17:26,  1.10s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  83% 4420/5352 [1:20:44<17:01,  1.10s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  83% 4440/5352 [1:20:52<16:36,  1.09s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  83% 4460/5352 [1:21:00<16:12,  1.09s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  84% 4480/5352 [1:21:08<15:47,  1.09s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  84% 4500/5352 [1:21:16<15:23,  1.08s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  84% 4520/5352 [1:21:24<14:59,  1.08s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  85% 4540/5352 [1:21:32<14:35,  1.08s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  85% 4560/5352 [1:21:40<14:11,  1.07s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  86% 4580/5352 [1:21:48<13:47,  1.07s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  86% 4600/5352 [1:21:56<13:23,  1.07s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  86% 4620/5352 [1:22:04<13:00,  1.07s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  87% 4640/5352 [1:22:12<12:36,  1.06s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  87% 4660/5352 [1:22:20<12:13,  1.06s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  87% 4680/5352 [1:22:29<11:50,  1.06s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  88% 4700/5352 [1:22:37<11:27,  1.05s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  88% 4720/5352 [1:22:45<11:04,  1.05s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  89% 4740/5352 [1:22:53<10:42,  1.05s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  89% 4760/5352 [1:23:01<10:19,  1.05s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  89% 4780/5352 [1:23:09<09:57,  1.04s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  90% 4800/5352 [1:23:17<09:34,  1.04s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  90% 4820/5352 [1:23:25<09:12,  1.04s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  90% 4840/5352 [1:23:33<08:50,  1.04s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  91% 4860/5352 [1:23:41<08:28,  1.03s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  91% 4880/5352 [1:23:49<08:06,  1.03s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  92% 4900/5352 [1:23:57<07:44,  1.03s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  92% 4920/5352 [1:24:05<07:23,  1.03s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  92% 4940/5352 [1:24:13<07:01,  1.02s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  93% 4960/5352 [1:24:21<06:40,  1.02s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  93% 4980/5352 [1:24:29<06:18,  1.02s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  93% 5000/5352 [1:24:38<05:57,  1.02s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  94% 5020/5352 [1:24:46<05:36,  1.01s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  94% 5040/5352 [1:24:54<05:15,  1.01s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  95% 5060/5352 [1:25:02<04:54,  1.01s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  95% 5080/5352 [1:25:10<04:33,  1.01s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  95% 5100/5352 [1:25:18<04:12,  1.00s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  96% 5120/5352 [1:25:26<03:52,  1.00s/it, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  96% 5140/5352 [1:25:34<03:31,  1.00it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  96% 5160/5352 [1:25:42<03:11,  1.00it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  97% 5180/5352 [1:25:50<02:51,  1.01it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  97% 5200/5352 [1:25:58<02:30,  1.01it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  98% 5220/5352 [1:26:06<02:10,  1.01it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  98% 5240/5352 [1:26:14<01:50,  1.01it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  98% 5260/5352 [1:26:23<01:30,  1.01it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  99% 5280/5352 [1:26:31<01:10,  1.02it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  99% 5300/5352 [1:26:39<00:51,  1.02it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5:  99% 5320/5352 [1:26:47<00:31,  1.02it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Epoch 5: 100% 5340/5352 [1:26:55<00:11,  1.02it/s, loss=1.23, v_num=5, train_loss=1.320, val_loss=1.360]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:04<00:04,  2.50it/s]\u001b[A\n",
            "Epoch 5: 100% 5352/5352 [1:27:00<00:00,  1.03it/s, loss=1.24, v_num=5, train_loss=1.200, val_loss=1.350]\n",
            "Epoch 5: 100% 5352/5352 [1:27:00<00:00,  1.03it/s, loss=1.24, v_num=5, train_loss=1.200, val_loss=1.350]Epoch 5, global step 25686: 'val_loss' reached 1.34756 (best 1.34756), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=05-val_loss=1.348.ckpt' as top 3\n",
            "Epoch 6:  80% 4280/5352 [1:20:02<20:02,  1.12s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  80% 4300/5352 [1:20:10<19:36,  1.12s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  81% 4320/5352 [1:20:18<19:11,  1.12s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  81% 4340/5352 [1:20:26<18:45,  1.11s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  81% 4360/5352 [1:20:34<18:19,  1.11s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  82% 4380/5352 [1:20:42<17:54,  1.11s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  82% 4400/5352 [1:20:50<17:29,  1.10s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  83% 4420/5352 [1:20:58<17:04,  1.10s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  83% 4440/5352 [1:21:06<16:39,  1.10s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  83% 4460/5352 [1:21:14<16:14,  1.09s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  84% 4480/5352 [1:21:22<15:50,  1.09s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  84% 4500/5352 [1:21:30<15:25,  1.09s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  84% 4520/5352 [1:21:38<15:01,  1.08s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  85% 4540/5352 [1:21:46<14:37,  1.08s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  85% 4560/5352 [1:21:55<14:13,  1.08s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  86% 4580/5352 [1:22:03<13:49,  1.07s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  86% 4600/5352 [1:22:11<13:26,  1.07s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  86% 4620/5352 [1:22:19<13:02,  1.07s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  87% 4640/5352 [1:22:27<12:39,  1.07s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  87% 4660/5352 [1:22:35<12:15,  1.06s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  87% 4680/5352 [1:22:43<11:52,  1.06s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  88% 4700/5352 [1:22:51<11:29,  1.06s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  88% 4720/5352 [1:22:59<11:06,  1.06s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  89% 4740/5352 [1:23:07<10:43,  1.05s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  89% 4760/5352 [1:23:15<10:21,  1.05s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  89% 4780/5352 [1:23:23<09:58,  1.05s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  90% 4800/5352 [1:23:31<09:36,  1.04s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  90% 4820/5352 [1:23:39<09:14,  1.04s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  90% 4840/5352 [1:23:48<08:51,  1.04s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  91% 4860/5352 [1:23:56<08:29,  1.04s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  91% 4880/5352 [1:24:04<08:07,  1.03s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  92% 4900/5352 [1:24:12<07:46,  1.03s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  92% 4920/5352 [1:24:20<07:24,  1.03s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  92% 4940/5352 [1:24:28<07:02,  1.03s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  93% 4960/5352 [1:24:36<06:41,  1.02s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  93% 4980/5352 [1:24:44<06:19,  1.02s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  93% 5000/5352 [1:24:52<05:58,  1.02s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  94% 5020/5352 [1:25:00<05:37,  1.02s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  94% 5040/5352 [1:25:08<05:16,  1.01s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  95% 5060/5352 [1:25:16<04:55,  1.01s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  95% 5080/5352 [1:25:24<04:34,  1.01s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  95% 5100/5352 [1:25:33<04:13,  1.01s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  96% 5120/5352 [1:25:41<03:52,  1.00s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  96% 5140/5352 [1:25:49<03:32,  1.00s/it, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  96% 5160/5352 [1:25:57<03:11,  1.00it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  97% 5180/5352 [1:26:05<02:51,  1.00it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  97% 5200/5352 [1:26:13<02:31,  1.01it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  98% 5220/5352 [1:26:21<02:11,  1.01it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  98% 5240/5352 [1:26:29<01:50,  1.01it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  98% 5260/5352 [1:26:37<01:30,  1.01it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  99% 5280/5352 [1:26:45<01:10,  1.01it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  99% 5300/5352 [1:26:53<00:51,  1.02it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6:  99% 5320/5352 [1:27:01<00:31,  1.02it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Epoch 6: 100% 5340/5352 [1:27:09<00:11,  1.02it/s, loss=1.27, v_num=5, train_loss=1.270, val_loss=1.350]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:07<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 6: 100% 5352/5352 [1:27:14<00:00,  1.02it/s, loss=1.24, v_num=5, train_loss=0.728, val_loss=1.340]\n",
            "Epoch 6: 100% 5352/5352 [1:27:14<00:00,  1.02it/s, loss=1.24, v_num=5, train_loss=0.728, val_loss=1.340]Epoch 6, global step 29967: 'val_loss' reached 1.33902 (best 1.33902), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=06-val_loss=1.339.ckpt' as top 3\n",
            "Epoch 7:  80% 4280/5352 [1:20:00<20:02,  1.12s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  80% 4300/5352 [1:20:08<19:36,  1.12s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  81% 4320/5352 [1:20:16<19:10,  1.11s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  81% 4340/5352 [1:20:24<18:45,  1.11s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  81% 4360/5352 [1:20:32<18:19,  1.11s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  82% 4380/5352 [1:20:40<17:54,  1.11s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  82% 4400/5352 [1:20:48<17:29,  1.10s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  83% 4420/5352 [1:20:56<17:04,  1.10s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  83% 4440/5352 [1:21:05<16:39,  1.10s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  83% 4460/5352 [1:21:13<16:14,  1.09s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  84% 4480/5352 [1:21:21<15:50,  1.09s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  84% 4500/5352 [1:21:29<15:25,  1.09s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  84% 4520/5352 [1:21:37<15:01,  1.08s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  85% 4540/5352 [1:21:45<14:37,  1.08s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  85% 4560/5352 [1:21:53<14:13,  1.08s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  86% 4580/5352 [1:22:01<13:49,  1.07s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  86% 4600/5352 [1:22:09<13:25,  1.07s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  86% 4620/5352 [1:22:17<13:02,  1.07s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  87% 4640/5352 [1:22:25<12:38,  1.07s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  87% 4660/5352 [1:22:33<12:15,  1.06s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  87% 4680/5352 [1:22:41<11:52,  1.06s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  88% 4700/5352 [1:22:50<11:29,  1.06s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  88% 4720/5352 [1:22:58<11:06,  1.05s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  89% 4740/5352 [1:23:06<10:43,  1.05s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  89% 4760/5352 [1:23:14<10:21,  1.05s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  89% 4780/5352 [1:23:22<09:58,  1.05s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  90% 4800/5352 [1:23:30<09:36,  1.04s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  90% 4820/5352 [1:23:38<09:13,  1.04s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  90% 4840/5352 [1:23:46<08:51,  1.04s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  91% 4860/5352 [1:23:54<08:29,  1.04s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  91% 4880/5352 [1:24:02<08:07,  1.03s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  92% 4900/5352 [1:24:10<07:45,  1.03s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  92% 4920/5352 [1:24:18<07:24,  1.03s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  92% 4940/5352 [1:24:26<07:02,  1.03s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  93% 4960/5352 [1:24:35<06:41,  1.02s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  93% 4980/5352 [1:24:43<06:19,  1.02s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  93% 5000/5352 [1:24:51<05:58,  1.02s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  94% 5020/5352 [1:24:59<05:37,  1.02s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  94% 5040/5352 [1:25:07<05:16,  1.01s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  95% 5060/5352 [1:25:15<04:55,  1.01s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  95% 5080/5352 [1:25:23<04:34,  1.01s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  95% 5100/5352 [1:25:31<04:13,  1.01s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  96% 5120/5352 [1:25:39<03:52,  1.00s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  96% 5140/5352 [1:25:47<03:32,  1.00s/it, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  96% 5160/5352 [1:25:55<03:11,  1.00it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  97% 5180/5352 [1:26:03<02:51,  1.00it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  97% 5200/5352 [1:26:11<02:31,  1.01it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  98% 5220/5352 [1:26:20<02:10,  1.01it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  98% 5240/5352 [1:26:28<01:50,  1.01it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  98% 5260/5352 [1:26:36<01:30,  1.01it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  99% 5280/5352 [1:26:44<01:10,  1.01it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  99% 5300/5352 [1:26:52<00:51,  1.02it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7:  99% 5320/5352 [1:27:00<00:31,  1.02it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Epoch 7: 100% 5340/5352 [1:27:08<00:11,  1.02it/s, loss=1.22, v_num=5, train_loss=1.100, val_loss=1.340]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:07<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 7: 100% 5352/5352 [1:27:13<00:00,  1.02it/s, loss=1.22, v_num=5, train_loss=1.050, val_loss=1.330]\n",
            "Epoch 7: 100% 5352/5352 [1:27:13<00:00,  1.02it/s, loss=1.22, v_num=5, train_loss=1.050, val_loss=1.330]Epoch 7, global step 34248: 'val_loss' reached 1.33460 (best 1.33460), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=07-val_loss=1.335.ckpt' as top 3\n",
            "Epoch 8:  80% 4280/5352 [1:20:07<20:04,  1.12s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  80% 4300/5352 [1:20:15<19:38,  1.12s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  81% 4320/5352 [1:20:23<19:12,  1.12s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  81% 4340/5352 [1:20:31<18:46,  1.11s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  81% 4360/5352 [1:20:40<18:21,  1.11s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  82% 4380/5352 [1:20:48<17:55,  1.11s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  82% 4400/5352 [1:20:56<17:30,  1.10s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  83% 4420/5352 [1:21:04<17:05,  1.10s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  83% 4440/5352 [1:21:12<16:40,  1.10s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  83% 4460/5352 [1:21:20<16:16,  1.09s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  84% 4480/5352 [1:21:28<15:51,  1.09s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  84% 4500/5352 [1:21:36<15:27,  1.09s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  84% 4520/5352 [1:21:44<15:02,  1.09s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  85% 4540/5352 [1:21:52<14:38,  1.08s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  85% 4560/5352 [1:22:00<14:14,  1.08s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  86% 4580/5352 [1:22:08<13:50,  1.08s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  86% 4600/5352 [1:22:16<13:27,  1.07s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  86% 4620/5352 [1:22:25<13:03,  1.07s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  87% 4640/5352 [1:22:33<12:40,  1.07s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  87% 4660/5352 [1:22:41<12:16,  1.06s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  87% 4680/5352 [1:22:49<11:53,  1.06s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  88% 4700/5352 [1:22:57<11:30,  1.06s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  88% 4720/5352 [1:23:05<11:07,  1.06s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  89% 4740/5352 [1:23:13<10:44,  1.05s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  89% 4760/5352 [1:23:21<10:22,  1.05s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  89% 4780/5352 [1:23:29<09:59,  1.05s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  90% 4800/5352 [1:23:37<09:37,  1.05s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  90% 4820/5352 [1:23:45<09:14,  1.04s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  90% 4840/5352 [1:23:53<08:52,  1.04s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  91% 4860/5352 [1:24:02<08:30,  1.04s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  91% 4880/5352 [1:24:10<08:08,  1.03s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  92% 4900/5352 [1:24:18<07:46,  1.03s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  92% 4920/5352 [1:24:26<07:24,  1.03s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  92% 4940/5352 [1:24:34<07:03,  1.03s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  93% 4960/5352 [1:24:42<06:41,  1.02s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  93% 4980/5352 [1:24:50<06:20,  1.02s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  93% 5000/5352 [1:24:58<05:58,  1.02s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  94% 5020/5352 [1:25:06<05:37,  1.02s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  94% 5040/5352 [1:25:14<05:16,  1.01s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  95% 5060/5352 [1:25:22<04:55,  1.01s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  95% 5080/5352 [1:25:30<04:34,  1.01s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  95% 5100/5352 [1:25:38<04:13,  1.01s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  96% 5120/5352 [1:25:47<03:53,  1.01s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  96% 5140/5352 [1:25:55<03:32,  1.00s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  96% 5160/5352 [1:26:03<03:12,  1.00s/it, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  97% 5180/5352 [1:26:11<02:51,  1.00it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  97% 5200/5352 [1:26:19<02:31,  1.00it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  98% 5220/5352 [1:26:27<02:11,  1.01it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  98% 5240/5352 [1:26:35<01:51,  1.01it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  98% 5260/5352 [1:26:43<01:31,  1.01it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  99% 5280/5352 [1:26:51<01:11,  1.01it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  99% 5300/5352 [1:26:59<00:51,  1.02it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8:  99% 5320/5352 [1:27:07<00:31,  1.02it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Epoch 8: 100% 5340/5352 [1:27:15<00:11,  1.02it/s, loss=1.22, v_num=5, train_loss=1.260, val_loss=1.330]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:08<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 8: 100% 5352/5352 [1:27:20<00:00,  1.02it/s, loss=1.22, v_num=5, train_loss=1.390, val_loss=1.330]\n",
            "Epoch 8: 100% 5352/5352 [1:27:20<00:00,  1.02it/s, loss=1.22, v_num=5, train_loss=1.390, val_loss=1.330]Epoch 8, global step 38529: 'val_loss' reached 1.33316 (best 1.33316), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=08-val_loss=1.333.ckpt' as top 3\n",
            "Epoch 9:  80% 4280/5352 [1:20:04<20:03,  1.12s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Validation: 0it [00:00, ?it/s]\u001b[A\n",
            "Validation:   0% 0/1071 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  80% 4300/5352 [1:20:12<19:37,  1.12s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  81% 4320/5352 [1:20:20<19:11,  1.12s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  81% 4340/5352 [1:20:28<18:45,  1.11s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  81% 4360/5352 [1:20:36<18:20,  1.11s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  82% 4380/5352 [1:20:44<17:55,  1.11s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  82% 4400/5352 [1:20:52<17:29,  1.10s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  83% 4420/5352 [1:21:00<17:04,  1.10s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  83% 4440/5352 [1:21:08<16:40,  1.10s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  83% 4460/5352 [1:21:16<16:15,  1.09s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  84% 4480/5352 [1:21:25<15:50,  1.09s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  84% 4500/5352 [1:21:33<15:26,  1.09s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  84% 4520/5352 [1:21:41<15:02,  1.08s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  85% 4540/5352 [1:21:49<14:38,  1.08s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  85% 4560/5352 [1:21:57<14:14,  1.08s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  86% 4580/5352 [1:22:05<13:50,  1.08s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  86% 4600/5352 [1:22:13<13:26,  1.07s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  86% 4620/5352 [1:22:21<13:02,  1.07s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  87% 4640/5352 [1:22:29<12:39,  1.07s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  87% 4660/5352 [1:22:37<12:16,  1.06s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  87% 4680/5352 [1:22:45<11:53,  1.06s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  88% 4700/5352 [1:22:53<11:29,  1.06s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  88% 4720/5352 [1:23:01<11:07,  1.06s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  89% 4740/5352 [1:23:09<10:44,  1.05s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  89% 4760/5352 [1:23:17<10:21,  1.05s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  89% 4780/5352 [1:23:26<09:59,  1.05s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  90% 4800/5352 [1:23:34<09:36,  1.04s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  90% 4820/5352 [1:23:42<09:14,  1.04s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  90% 4840/5352 [1:23:50<08:52,  1.04s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  91% 4860/5352 [1:23:58<08:30,  1.04s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  91% 4880/5352 [1:24:06<08:08,  1.03s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  92% 4900/5352 [1:24:14<07:46,  1.03s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  92% 4920/5352 [1:24:22<07:24,  1.03s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  92% 4940/5352 [1:24:30<07:02,  1.03s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  93% 4960/5352 [1:24:38<06:41,  1.02s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  93% 4980/5352 [1:24:46<06:19,  1.02s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  93% 5000/5352 [1:24:54<05:58,  1.02s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  94% 5020/5352 [1:25:02<05:37,  1.02s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  94% 5040/5352 [1:25:10<05:16,  1.01s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  95% 5060/5352 [1:25:19<04:55,  1.01s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  95% 5080/5352 [1:25:27<04:34,  1.01s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  95% 5100/5352 [1:25:35<04:13,  1.01s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  96% 5120/5352 [1:25:43<03:53,  1.00s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  96% 5140/5352 [1:25:51<03:32,  1.00s/it, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  96% 5160/5352 [1:25:59<03:11,  1.00it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  97% 5180/5352 [1:26:07<02:51,  1.00it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  97% 5200/5352 [1:26:15<02:31,  1.00it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  98% 5220/5352 [1:26:23<02:11,  1.01it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  98% 5240/5352 [1:26:31<01:50,  1.01it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  98% 5260/5352 [1:26:39<01:30,  1.01it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  99% 5280/5352 [1:26:47<01:11,  1.01it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  99% 5300/5352 [1:26:55<00:51,  1.02it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9:  99% 5320/5352 [1:27:03<00:31,  1.02it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Epoch 9: 100% 5340/5352 [1:27:12<00:11,  1.02it/s, loss=1.26, v_num=5, train_loss=1.490, val_loss=1.330]\n",
            "Validation DataLoader 0:  99% 1060/1071 [07:07<00:04,  2.48it/s]\u001b[A\n",
            "Epoch 9: 100% 5352/5352 [1:27:16<00:00,  1.02it/s, loss=1.25, v_num=5, train_loss=1.190, val_loss=1.330]\n",
            "Epoch 9: 100% 5352/5352 [1:27:16<00:00,  1.02it/s, loss=1.25, v_num=5, train_loss=1.190, val_loss=1.330]Epoch 9, global step 42810: 'val_loss' reached 1.32581 (best 1.32581), saving model to '/content/KoBART-summarization/logs/model_chp/epoch=09-val_loss=1.326.ckpt' as top 3\n",
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "Epoch 9: 100% 5352/5352 [1:27:30<00:00,  1.02it/s, loss=1.25, v_num=5, train_loss=1.190, val_loss=1.330]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "\n",
        "def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "\n",
        "\n",
        "clear_output()\n",
        "inf('\\u2714 Done','success', '50px')"
      ],
      "metadata": {
        "id": "kgjur4V67pOS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "19de5d89ec96449ba4f99d382fb45657",
            "11a5dd52e0994989b5370c880c29da79",
            "8507a59e70624bf89091086c989e5f05"
          ]
        },
        "outputId": "eae06e1d-4109-451d-e2e7-b18bccec93b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19de5d89ec96449ba4f99d382fb45657"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐️⭐️ make model.bin"
      ],
      "metadata": {
        "id": "2U0UsGRTOmRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/KoBART-summarization\n",
        "cmd = '''python get_model_binary.py --hparams /content/KoBART-summarization/logs/tb_logs/lightning_logs/version_0/hparams.yaml \\\n",
        "                                 --model_binary /content/KoBART-summarization/logs/model_chp/your_model_name.ckpt \\\n",
        "                                 --output_dir /content/KoBART-summarization/model \\\n",
        "'''\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "DFhRWs-Q7pWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace88580-af02-448a-984e-19b08eacf911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoBART-summarization\n",
            "2023-03-30 21:26:12.234236: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-30 21:26:12.234329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-30 21:26:12.234346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGcsBs637pk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⭐️⭐️Well Done⭐️⭐️\n",
        "\n",
        "- go KoBART_summarization_colab_debug.ipynb now\n",
        "\n",
        "- [KoBART_summarization_colab_debug.ipynb](https://github.com/ugiugi0823/KoBART_summarization_colab_debug)"
      ],
      "metadata": {
        "id": "j9PkdOsGRdRv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hf-3TjSa7poD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_gXXCti7pqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o4fnzQeW7psf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9F2rfb1z7pvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2mB4XbvfUfQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip freeze > requirements2.txt"
      ],
      "metadata": {
        "id": "uN_8fXim5f-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}